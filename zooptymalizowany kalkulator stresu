import numpy as np
import pandas as pd
import os
import joblib
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_validate, RandomizedSearchCV
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import balanced_accuracy_score, f1_score, make_scorer, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.inspection import permutation_importance


class Config:
    excel_path: str = "prawidowy_excel.xlsx"
    sheet_name: str | None = None
    stress_high_threshold: float = 8.0
    test_size: float = 0.35
    random_state: int = 42

'''
    Klasa przechowująca ustawienia konfiguracyjne, takie jak ścieżki do plików, parametry podziału danych,
    próg stresu oraz ziarno losowe.

    Attributes:
    excel_path (str): Ścieżka do pliku Excel, z którego będą wczytywane dane.
    sheet_name (str | None): Nazwa arkusza w pliku Excel. Jeśli None, używany jest domyślny arkusz.
    stress_high_threshold (float): Próg, powyżej którego poziom stresu będzie uznawany za "HIGH".
    test_size (float): Procentowy udział danych przeznaczonych na test (np. 0.35 oznacza 35%).
    random_state (int): Ziarno losowe do zapewnienia powtarzalności wyników.
'''
cfg = Config()

def find_stress_column(df: pd.DataFrame) -> str:

    # Zamienia wszystkie nazwy kolumn na małe litery
    cols_lower = {c: str(c).lower() for c in df.columns}

      # Znajduje kolumny, które zawierają słowo "stres
    stress_candidates = [c for c in df.columns if "stres" in cols_lower[c]]
    if not stress_candidates:
        raise ValueError("Nie znaleziono kolumny stresu.")
    for c in stress_candidates:
        cl = cols_lower[c]
        if "poziom" in cl or "skali" in cl or "skala" in cl:
            return c
    return stress_candidates[0]
'''
    Funkcja znajduje kolumnę w DataFrame zawierającą dane dotyczące poziomu stresu.
    Funkcja sprawdza, czy któraś z kolumn zawiera słowo 'stres', a następnie,
    w przypadku wielu pasujących kolumn, stara się wybrać tę, która zawiera
    dodatkowo 'poziom', 'skala' lub 'skali'.

    Args:
        df (pd.DataFrame): DataFrame, który zawiera dane, w którym szukamy kolumny
                           związanej ze stresem.

    Returns:
        str: Nazwa kolumny zawierającej dane o stresie.

    Raises:
        ValueError: Jeśli w DataFrame nie znaleziono żadnej kolumny związanej ze stresem.

'''
def coerce_numeric_commas(series: pd.Series) -> pd.Series:

    # Zamienia przecinki na kropki w liczbach zapisanych w formacie tekstowym
    s = series.astype(str).str.replace(",", ".", regex=False).str.strip()

     # Zamienia wartości tekstowe, takie jak "nan", "None" itp. na NaN
    s = s.replace({"nan": np.nan, "None": np.nan, "none": np.nan, "": np.nan})

     # Konwertuje ciągi tekstowe na liczby zmiennoprzecinkowe, ignorując błędy (zwracając NaN, jeśli nie uda się przekonwertować)

    return pd.to_numeric(s, errors="coerce")

'''
    Funkcja konwertuje wartości tekstowe zawierające liczby z przecinkiem (np. "1,5") na liczby zmiennoprzecinkowe,
    a także zamienia puste komórki i wartości takie jak "nan", "None", "none" na wartości `NaN`.

    Args:
        series (pd.Series): Seria danych, która zawiera wartości tekstowe reprezentujące liczby.

    Returns:
        pd.Series: Seria danych, w której wartości zostały przekonwertowane na liczby zmiennoprzecinkowe,
                   a niemożliwe do konwersji wartości zostały zamienione na `NaN`.
'''



def make_target_from_stress(stress_num: pd.Series, thr: float) -> pd.Series:
    return pd.Series(np.where(stress_num >= thr, "HIGH", "NOT_HIGH"), index=stress_num.index)
'''
    Funkcja tworzy zmienną celu (target) na podstawie wartości stresu.
    Przypisuje wartość "HIGH", jeśli wartość stresu jest większa lub równa zadanemu progowi (thr),
    w przeciwnym razie przypisuje wartość "NOT_HIGH".

    Args:
        stress_num (pd.Series): Seria z wartościami stresu (numerycznymi).
        thr (float): Próg stresu, powyżej którego klasyfikacja jest uznawana za "HIGH".

    Returns:
        pd.Series: Seria z wartościami "HIGH" lub "NOT_HIGH", zależnie od wartości stresu i progu.
'''

def make_preprocess():
    # Selektor kolumn numerycznych
    num_sel = make_column_selector(dtype_include=np.number)

    # Selektor kolumn kategorycznych
    cat_sel = make_column_selector(dtype_exclude=np.number)

    # Pipeline dla kolumn numerycznych: imputacja wartości brakujących (zamiast ich usuwania, wstawiamy medianę)
    num_pipe = Pipeline([("imp", SimpleImputer(strategy="median"))])

     # Pipeline dla kolumn kategorycznych: imputacja najczęściej występującą wartością oraz kodowanie OneHot
    cat_pipe = Pipeline(
        [("imp", SimpleImputer(strategy="most_frequent")), ("ohe", OneHotEncoder(handle_unknown="ignore"))])

    # ColumnTransformer do zastosowania różnych operacji dla kolumn numerycznych i kategorycznych
    return ColumnTransformer(
        transformers=[("num", num_pipe, num_sel), ("cat", cat_pipe, cat_sel)],
        remainder="drop" #inne kolumny są pomijane
    )

'''
    Funkcja tworzy pipeline do przetwarzania danych przed ich użyciem w modelu.
    Przetwarza dane liczbowe za pomocą imputacji wartości brakujących (mediana),
    a dane kategoryczne imputuje wartości brakujące na podstawie najczęściej występującej wartości
    i koduje je przy pomocy OneHotEncoder.

    Args:
        Brak argumentów.

    Returns:
        ColumnTransformer: Obiekt, który wykonuje różne operacje na kolumnach numerycznych i kategorycznych.
                            Dla kolumn numerycznych - imputacja mediana, dla kolumn kategorycznych - imputacja
                            najczęstszej wartości i kodowanie przy pomocy OneHotEncoder.
'''



def load_data(excel_path: str):

    if not os.path.exists(excel_path):
        raise FileNotFoundError(f"Nie widzę pliku: {excel_path}.")

        # Wczytanie danych z pliku Excel, domyślnie z pierwszego arkusza
    raw = pd.read_excel(excel_path, sheet_name=cfg.sheet_name, engine="openpyxl")

    # Jeśli wczytano słownik arkuszy, wybieramy pierwszy arkusz, w przeciwnym razie po prostu kopiujemy dane
    df = raw[list(raw.keys())[0]].copy() if isinstance(raw, dict) else raw.copy()

     # Usuwamy puste wiersze
    df = df.dropna(how="all")

    return df

'''
    Funkcja wczytuje dane z pliku Excel na podstawie podanej ścieżki. Oczekuje, że plik zawiera dane w arkuszu,
    którego nazwę można określić w `cfg.sheet_name`. Wczytane dane są oczyszczane z pustych wierszy.

    Args:
        excel_path (str): Ścieżka do pliku Excel, który ma zostać wczytany.

    Returns:
        pd.DataFrame: Wczytany DataFrame z danymi.

    Raises:
        FileNotFoundError: Jeśli plik wskazany w `excel_path` nie istnieje.
'''


# Wczytanie danych z pliku Excel
df = load_data(cfg.excel_path)

# Znalezienie kolumny stresu w danych
stress_col = find_stress_column(df)

# Konwersja wartości w kolumnie stresu na liczby
stress_num = coerce_numeric_commas(df[stress_col])

# Tworzenie maski dla danych z niepustymi wartościami w kolumnie stresu
mask = stress_num.notna()

# Oczyszczanie danych, pozostawienie tylko wierszy z wartościami stresu
df = df.loc[mask].copy()
stress_num = stress_num.loc[mask].copy()

# Generowanie zmiennej celu (target) na podstawie poziomu stresu
y = make_target_from_stress(stress_num, cfg.stress_high_threshold)
y.name = "stress_class"

# Usuwanie kolumny stresu z danych wejściowych (X)
X = df.drop(columns=[stress_col], errors="ignore").copy()

# Konwersja kolumn tekstowych na liczby, jeśli zawierają wartości liczbowe (>= 70%)
for c in X.columns:
    if X[c].dtype == "object":
        maybe = coerce_numeric_commas(X[c])
        if maybe.notna().mean() >= 0.70:
            X[c] = maybe

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=cfg.test_size, random_state=cfg.random_state, stratify=y
)

'''
    Funkcja `train_test_split` dzieli dane na zestaw treningowy i testowy. Jest to część procesu przygotowywania
    danych do modelowania, gdzie dane wejściowe (X) oraz zmienna celu (y) są dzielone na dwie części:
    - Zestaw treningowy, który służy do uczenia modelu.
    - Zestaw testowy, który służy do oceny wydajności modelu.

    Args:
        X: pd.DataFrame - Dane wejściowe (cechy).
        y: pd.Series - Zmienna celu (etykieta).
        test_size: float - Procentowy udział zestawu testowego (np. 0.35 oznacza 35% danych do testów).
        random_state: int - Ziarno generatora liczb losowych, aby wyniki były powtarzalne.
        stratify: pd.Series - Zmienna do próbkowania warstwowego (zachowuje proporcje klas w zbiorach).

    Returns:
        X_train, X_test, y_train, y_test:
            - X_train: pd.DataFrame - Zestaw treningowy danych wejściowych.
            - X_test: pd.DataFrame - Zestaw testowy danych wejściowych.
            - y_train: pd.Series - Zestaw treningowy zmiennej celu.
            - y_test: pd.Series - Zestaw testowy zmiennej celu.
'''


f1_high_scorer = make_scorer(f1_score, pos_label="HIGH")
'''
    Funkcja `make_scorer` z pakietu `sklearn` tworzy niestandardową miarę oceny modelu.
    W tym przypadku tworzony jest wskaźnik F1-score, który jest specjalnie zaprojektowany do klasyfikacji nierównych klas
    (z dużą przewagą jednej klasy). Wartość F1 jest miarą średnią harmoniczną precyzji i czułości.

    Args:
        f1_score: Funkcja do obliczania wskaźnika F1.
        pos_label: str - Etkieta pozytywnej klasy (w tym przypadku "HIGH"), dla której obliczany jest wskaźnik F1.

    Returns:
        f1_high_scorer: funkcja - Funkcja oceny, którą można wykorzystać w metodach takich jak cross-validation.
'''

pipe_lr = Pipeline([
    ("preprocess", make_preprocess()),
    ("model", LogisticRegression(max_iter=5000, class_weight="balanced", random_state=cfg.random_state))
])
'''
    Tworzenie pipeline'u dla modelu regresji logistycznej. Pipeline to obiekt w `sklearn`, który pozwala połączyć
    etapy przetwarzania danych (np. przekształcenie) z procesem uczenia modelu w jeden spójny proces.

    Args:
        make_preprocess(): Funkcja, która tworzy pipeline do przetwarzania danych, w tym imputacja brakujących wartości i kodowanie zmiennych kategorycznych.
        LogisticRegression: Model regresji logistycznej, wykorzystywany do klasyfikacji danych binarnych.

    Returns:
        pipe_lr: Pipeline - Złożony obiekt, który automatycznie przetwarza dane, a następnie uczy model.
'''


pipe_etc = Pipeline([
    ("preprocess", make_preprocess()),
    ("model", ExtraTreesClassifier(n_estimators=600, random_state=cfg.random_state, n_jobs=1, class_weight="balanced",
                                   min_samples_leaf=2))
])
'''
    Tworzenie pipeline'u dla modelu Extra Trees Classifier. Jest to model klasyfikacyjny, który działa na zasadzie lasu drzew decyzyjnych.
    Używa losowego zbioru drzew (tak jak Random Forest), ale z pewnymi optymalizacjami.

    Args:
        make_preprocess(): Funkcja, która tworzy pipeline do przetwarzania danych (np. imputacja, kodowanie).
        ExtraTreesClassifier: Model klasyfikacyjny bazujący na wielu drzewach decyzyjnych (ensamble learning),
                              który jest bardziej odporny na overfitting w porównaniu do pojedynczego drzewa.

    Returns:
        pipe_etc: Pipeline - Złożony obiekt, który przetwarza dane, a następnie trenuje model klasyfikacji.
'''


def eval_holdout(pipe, name):
    pipe.fit(X_train, y_train)
    pred = pipe.predict(X_test)

    ba = balanced_accuracy_score(y_test, pred)
    f1h = f1_score(y_test, pred, pos_label="HIGH")
    cm = confusion_matrix(y_test, pred, labels=["HIGH", "NOT_HIGH"])

    print(f"{name} — HOLDOUT")
    print(f"BA: {ba:.4f} | F1(HIGH): {f1h:.4f}")
    print(f"CM:\n{cm}")
    print(classification_report(y_test, pred))


eval_holdout(pipe_lr, "LogisticRegression")
eval_holdout(pipe_etc, "ExtraTreesClassifier")
'''
  Ocena modelu na zestawie testowym przy użyciu miar takich jak Balanced Accuracy, F1-score (dla klasy "HIGH") i macierzy pomyłek.

  Arguments:
      pipe (Pipeline): Pipeline zawierający kroki przetwarzania danych oraz model klasyfikacyjny.
      name (str): Nazwa modelu (np. 'LogisticRegression' lub 'ExtraTreesClassifier').

  Returns:
      None: Funkcja wypisuje wyniki na konsoli, ale nic nie zwraca.
 '''


param_dist = {
  "model__n_estimators": [400, 800, 1200],
  "model__max_depth": [None, 5, 8, 12, 16, 20],
  "model__min_samples_split": [2, 5, 10, 20, 40],
  "model__min_samples_leaf": [1, 2, 4, 8, 12, 20],
  "model__max_features": ["sqrt", "log2", 0.3, 0.5, 0.8, None],
  "model__bootstrap": [False, True],
  "model__class_weight": [None, "balanced"],
}
'''
  Słownik parametrów do strojenia hiperparametrów dla modelu ExtraTreesClassifier. Jest używany w RandomizedSearchCV.
  Zawiera parametry do wyboru liczby drzew, maksymalnej głębokości drzew, liczby próbek w liściu itp.

  Arguments:
      model__n_estimators (list): Lista liczby drzew w lesie.
      model__max_depth (list): Lista maksymalnych głębokości drzew.
      model__min_samples_split (list): Lista minimalnej liczby próbek do podziału węzła.
      model__min_samples_leaf (list): Lista minimalnej liczby próbek w liściu.
      model__max_features (list): Lista opcji dla liczby cech rozważanych przy podziale drzewa.
      model__bootstrap (list): Lista opcji dla używania bootstrapu (losowego próbkowania).
      model__class_weight (list): Lista opcji dla wagi klas w modelu.

  Returns:
      dict: Zwraca słownik parametrów do użycia w RandomizedSearchCV.
'''


search = RandomizedSearchCV(
  estimator=pipe_etc,
  param_distributions=param_dist,
  n_iter=40,
  scoring={"f1_high": f1_high_scorer},
  refit="f1_high",
  cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=cfg.random_state),
  n_jobs=1,
  random_state=cfg.random_state,
  verbose=1,
  return_train_score=False
)

search.fit(X, y)
best_pipe = search.best_estimator_
joblib.dump(best_pipe, "results/best_model.joblib")

'''
search = RandomizedSearchCV(
  Funkcja RandomizedSearchCV przeprowadza wyszukiwanie losowe po przestrzeni hiperparametrów dla modelu.
  Szukamy najlepszych parametrów, które maksymalizują miarę 'f1_high'. Przeprowadzamy cross-validation
  z użyciem powtarzanego podziału danych (RepeatedStratifiedKFold) oraz ocenę na podstawie wskaźnika F1 dla klasy 'HIGH'.

  Arguments:
      estimator (Pipeline): Pipeline modelu do strojenia.
      param_distributions (dict): Słownik z możliwymi wartościami hiperparametrów do wyszukiwania.
      n_iter (int): Liczba iteracji w poszukiwaniach losowych.
      scoring (dict): Miara oceny dla wyszukiwania (np. f1_high_scorer).
      refit (str): Parametr, który wskazuje, na jakim wskaźniku powinniśmy ponownie dopasować najlepszy model.
      cv (RepeatedStratifiedKFold): Krzyżowa walidacja z powtarzaniem, w celu oceny modelu.
      n_jobs (int): Liczba rdzeni procesora do równoległego obliczania.
      random_state (int): Ziarno generatora liczb losowych.
      verbose (int): Poziom szczegółowości komunikatów w trakcie wyszukiwania.
      return_train_score (bool): Jeśli True, zwraca wyniki na zbiorze treningowym.

  Returns:
      RandomizedSearchCV: Obiekt po wyszukaniu najlepszych parametrów.
'''


'''
search.fit(X, y)
  Dopasowuje model RandomizedSearchCV do danych treningowych X i y w celu znalezienia najlepszych parametrów modelu.

  Arguments:
      X (pd.DataFrame): Dane wejściowe (cechy).
      y (pd.Series): Dane wyjściowe (etykieta).

  Returns:
      None: Funkcja nie zwraca żadnej wartości, ale aktualizuje obiekt `search` o najlepszy model.
'''


'''
best_pipe = search.best_estimator_
  Zwraca najlepszy model z dopasowania (z najlepszymi hiperparametrami) wybrany na podstawie 'refit' (w tym przypadku f1_high).

  Returns:
      best_pipe (Pipeline): Najlepszy model z wyszukiwania hiperparametrów.
'''


'''
joblib.dump(best_pipe, "results/best_model.joblib")
  Zapisuje najlepszy model (best_pipe) do pliku 'results/best_model.joblib', aby można go było później załadować i używać.

  Arguments:
      best_pipe (Pipeline): Najlepszy model, który zostanie zapisany.

  Returns:
      None: Funkcja zapisuje obiekt modelu na dysku.
'''

def ask_option(question: str, options: list[str]) -> tuple[int, str]:
  legend = " ".join([f"[{i+1}={opt}]" for i, opt in enumerate(options)])
  base_prompt = f"{question} {legend}: "
  prompt = base_prompt


  while True:
      s = input(prompt).strip()   # Pobiera odpowiedź użytkownika i usuwa nadmiarowe spacje
      if s.isdigit(): # Sprawdza, czy odpowiedź jest liczbą
          k = int(s)
          if 1 <= k <= len(options): # Jeśli liczba mieści się w zakresie dostępnych opcji
              return k, options[k - 1]
      prompt = f"BŁĄD!: WPISZ NUMER Z PODANYCH. Pytanie: {base_prompt}" # Ponownie prosi o poprawną odpowiedź

'''
  Funkcja zadaje użytkownikowi pytanie z listą opcji i oczekuje, że użytkownik wybierze jedną z opcji.
  Jeśli użytkownik poda niewłaściwą odpowiedź (np. nie wpisując liczby lub wpisując liczbę spoza dostępnych opcji),
  funkcja będzie ponownie prosić o poprawną odpowiedź.

  Args:
      question (str): Pytanie, które ma zostać wyświetlone użytkownikowi.
      options (list[str]): Lista dostępnych opcji, z której użytkownik wybiera jedną.

  Returns:
      tuple[int, str]: Zwraca krotkę, gdzie pierwszy element to numer wybranej opcji (int),
                       a drugi to sama wybrana opcja (str).

  Raises:
      ValueError: Jeśli użytkownik nie wybierze poprawnej opcji (np. wybór poza zakresem dostępnych opcji).
 '''



def risk_level(p_high: float) -> str:
  if p_high < 0.20:
      return "niskie"
  if p_high < 0.40:
      return "umiarkowane"
  if p_high < 0.60:
      return "podwyższone"
  return "wysokie"

'''
  Funkcja ocenia poziom ryzyka na podstawie prawdopodobieństwa (p_high) wystąpienia wysokiego stresu.
  Na podstawie wartości `p_high` zwraca odpowiednią kategorię ryzyka.

  Args:
      p_high (float): Prawdopodobieństwo wystąpienia wysokiego stresu (w przedziale od 0 do 1).

  Returns:
      str: Kategoria ryzyka, która może przyjąć jedną z wartości:
          - "niskie" dla p_high < 0.20
          - "umiarkowane" dla 0.20 <= p_high < 0.40
          - "podwyższone" dla 0.40 <= p_high < 0.60
          - "wysokie" dla p_high >= 0.60
'''




def main():
  pipe = joblib.load("results/best_model.joblib")
  print("KALKULATOR: Predykcja wysokiego stresu (HIGH vs NOT_HIGH)\n")



  sleep_opts = ["Mniej niż 5", "5-6", "7-8", "Więcej niż 8"]
  caffeine_opts = ["0", "1", "2", "3", "4 lub więcej"]
  study_opts = ["Mniej niż 1 godzinę", "1-2 godziny", "3-4 godziny", "5 lub więcej"]
  exercise_opts = ["0", "1-2 dni", "3-4 dni", "5-6 dni", "Codziennie"]
  alc_opts = ["Nigdy", "Sporadycznie (raz w miesiącu lub rzadziej)", "Kilka razy w miesiącu",
              "Regularnie (kilka razy w tygodniu)"]
  smoke_opts = ["Nigdy", "Sporadycznie (np. przy okazji imprezy)", "Kilka razy w tygodniu", "Codziennie"]
  relax_opts = ["W ogóle (0 razy w miesiącu)", "Rzadko (1-2 razy w miesiącu)", "Kilka razy w miesiącu (3-5 razy)",
                "Często (6 lub więcej razy w miesiącu)"]

  SLEEP_MAP = {1: 4.5, 2: 5.5, 3: 7.5, 4: 8.5}
  CAFFEINE_MAP = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}
  STUDY_MAP = {1: 0.5, 2: 1.5, 3: 3.5, 4: 5.0}
  EXERCISE_MAP = {1: 0, 2: 1.5, 3: 3.5, 4: 5.5, 5: 7}
  ALC_MAP = {1: 1, 2: 2, 3: 3, 4: 4}
  SMOKE_MAP = {1: 1, 2: 2, 3: 3, 4: 4}
  RELAX_MAP = {1: 0.0, 2: 1.5, 3: 4.0, 4: 6.0}

  questions = [
      ("ile_godzin_spisz_srednio_na_dob", "1/7 Ile godzin śpisz średnio na dobę?", sleep_opts, SLEEP_MAP),
      ("ile_kaw_napojow_energetycznych_250_ml_spozywasz_w_ciagu_dnia",
       "2/7 Ile kaw/ napojów energetycznych (250 ml) spożywasz w ciągu dnia?", caffeine_opts, CAFFEINE_MAP),
      ("ile_ile_godzin_dziennie_poswiecasz_na_nauke", "3/7 Ile godzin dziennie poświęcasz na naukę?", study_opts,
       STUDY_MAP),
      ("ile_dni_w_tygodniu_cwiczysz", "4/7 Ile dni w tygodniu ćwiczysz?", exercise_opts, EXERCISE_MAP),
      ("jak_czesto_spozywasz_alkohol", "5/7 Jak często spożywasz alkohol?", alc_opts, ALC_MAP),
      ("jak_czesto_palisz_papierosy", "6/7 Jak często palisz papierosy?", smoke_opts, SMOKE_MAP),
      ("ile_razy_w_miesiacu_uczestniczysz_w_aktywnościach_odstresowujacych_npkino_zakupy_spacery_restauracja_kregle",
       "7/7 Ile razy w miesiącu uczestniczysz w aktywnościach odstresowujących?", relax_opts, RELAX_MAP)
  ]

  x = {}
  summary = []

  for col, q, opts, mapper in questions:
      k, label = ask_option(q, opts)
      x[col] = mapper[k]
      summary.append((q.split(" ", 1)[1], label))  # bez "1/7 "

  df = pd.DataFrame([x], columns=FEATURES)
  pred = pipe.predict(df)[0]
  p_high = None

  if hasattr(pipe, "predict_proba"):
      proba = pipe.predict_proba(df)[0]
      classes = list(pipe.classes_)
      if "HIGH" in classes:
          p_high = float(proba[classes.index("HIGH")])
      if USE_THRESHOLD and p_high is not None:
          pred = "HIGH" if p_high >= THRESHOLD else "NOT_HIGH"

  print("\nTwoje odpowiedzi:")
  for q, label in summary:
      print(f"- {q}: {label}")

  print(f"\nWynik: {pred}")
  if p_high is not None:
      print(f"Prawdopodobieństwo HIGH: {p_high:.3f}")
      if USE_THRESHOLD:
          print(f"Próg HIGH: {THRESHOLD:.2f}")
      print(f"Ocena ryzyka HIGH: {risk_level(p_high)}")

FEATURES = [
  "ile_godzin_spisz_srednio_na_dob",
  "ile_kaw_napojow_energetycznych_250_ml_spozywasz_w_ciagu_dnia",
  "ile_ile_godzin_dziennie_poswiecasz_na_nauke",
  "ile_dni_w_tygodniu_cwiczysz",
  "jak_czesto_spozywasz_alkohol",
  "jak_czesto_palisz_papierosy",
  "ile_razy_w_miesiacu_uczestniczysz_w_aktywnosciach_odstresowujacych_npkino_zakupy_spacery_restauracja_kregle",
]

if __name__ == "__main__":
  main()

'''
  Funkcja główna, która uruchamia kalkulator do predykcji wysokiego stresu (HIGH vs NOT_HIGH).
  Zawiera pytania dotyczące różnych aspektów życia, takich jak sen, kawa, nauka, ćwiczenia, alkohol, papierosy i relaks.
  Na podstawie odpowiedzi użytkownika oblicza prawdopodobieństwo wystąpienia wysokiego stresu oraz ocenia ryzyko.

  Funkcja wykonuje następujące kroki:
  1. Ładuje wytrenowany model za pomocą joblib (plik: "results/best_model.joblib").
  2. Zadaje użytkownikowi pytania dotyczące stylu życia.
  3. Mapuje odpowiedzi użytkownika na liczby, które są używane do predykcji.
  4. Na podstawie odpowiedzi oblicza wynik klasyfikacji: "HIGH" lub "NOT_HIGH".
  5. Oblicza prawdopodobieństwo wysokiego stresu (jeśli dostępne) i ocenia ryzyko.

  Args:
      Brak.

  Returns:
      None.
 '''
